const OpenAI = require('openai');
require('dotenv').config();

// ğŸ° BunnyEra OpenRouter Integration
// Documentation: https://openrouter.ai/docs

const openai = new OpenAI({
    baseURL: 'https://openrouter.ai/api/v1',
    apiKey: process.env.OPENROUTER_API_KEY || 'sk-or-your-key-here', // âš ï¸ å¿…é¡»é…ç½®
    defaultHeaders: {
        'HTTP-Referer': process.env.APP_URL || 'https://bunnyera.com', // åº”ç”¨ä¸»é¡µ
        'X-Title': process.env.APP_NAME || 'BunnyEra AI', // åº”ç”¨åç§°
    },
});

/**
 * å‘é€èŠå¤©è¯·æ±‚åˆ° OpenRouter
 * @param {string} model - æ¨¡å‹åç§° (å¦‚ 'openai/gpt-4o', 'google/gemini-pro-1.5')
 * @param {Array} messages - æ¶ˆæ¯å†å² [{ role: 'user', content: '...' }]
 * @param {Object} options - å…¶ä»–å‚æ•° (temperature, max_tokens ç­‰)
 * @returns {Promise<Object>} - è¿”å› message å¯¹è±¡ { role, content }
 */
async function chatCompletion(model, messages, options = {}) {
    try {
        console.log(`ğŸš€ [OpenRouter] Requesting model: ${model}`);
        const completion = await openai.chat.completions.create({
            model: model || 'openai/gpt-3.5-turbo',
            messages: messages,
            ...options
        });

        return completion.choices[0].message;
    } catch (error) {
        console.error('âŒ [OpenRouter] Error:', error.message);
        throw error;
    }
}

/**
 * è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
 */
async function getModels() {
    try {
        const list = await openai.models.list();
        return list.data;
    } catch (error) {
        console.error('âŒ [OpenRouter] Failed to fetch models:', error.message);
        return [];
    }
}

module.exports = {
    openai,
    chatCompletion,
    getModels
};
